# Simple Crawler

A lightweight, independent single-page web crawler with intelligent HTTP/browser fallback.

## Features

- ğŸš€ **Direct HTTP Fetch** - Fast, lightweight scraping for static pages
- ğŸŒ **Browser Rendering** - Automatic fallback to Playwright for JavaScript-heavy pages
- ğŸ›¡ï¸ **Anti-Detection** - Randomized User-Agent, request headers, and browser fingerprint masking
- ğŸ§¹ **Content Cleaning** - Removes headers, footers, navigation, ads, and other non-essential content
- ğŸ“ **Markdown Conversion** - Converts cleaned HTML to clean Markdown with GFM support
- ğŸ”— **Link Extraction** - Extracts and normalizes all page links
- ğŸ“Š **Metadata Extraction** - Extracts title, description, Open Graph data, etc.

## Installation

```bash
cd simple-crawler
pnpm install
npx playwright install chromium
```

## Usage

### Basic Usage

```typescript
import { scrape, cleanup } from 'simple-crawler';

// Scrape a single page
const result = await scrape('https://example.com');

console.log(result.markdown);  // Cleaned markdown content
console.log(result.links);     // Array of extracted links
console.log(result.metadata);  // Page metadata

// Clean up browser resources when done
await cleanup();
```

### With Options

```typescript
const result = await scrape('https://example.com', {
  // Timeout in milliseconds
  timeout: 30000,
  
  // Wait time after page load (for JS rendering)
  waitAfterLoad: 2000,
  
  // Only extract main content (removes nav, footer, etc.)
  onlyMainContent: true,
  
  // Force using browser instead of HTTP fetch
  forceBrowser: false,
  
  // Custom request headers
  headers: {
    'Accept-Language': 'zh-CN,zh;q=0.9',
  },
  
  // Include only specific elements (CSS selectors)
  includeTags: ['article', '.content'],
  
  // Exclude specific elements (CSS selectors)
  excludeTags: ['.comments', '.related-posts'],
});
```

### Scrape Multiple URLs

```typescript
import { scrapeMultiple, cleanup } from 'simple-crawler';

const results = await scrapeMultiple(
  ['https://example.com', 'https://news.ycombinator.com'],
  { timeout: 30000 },
  3  // concurrency
);

await cleanup();
```

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Input URL                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   HTTP Fetch Engine                     â”‚
â”‚  - Random User-Agent                                    â”‚
â”‚  - Realistic request headers                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Check for issues:     â”‚
            â”‚   - 403/429/503 status  â”‚
            â”‚   - Empty content       â”‚
            â”‚   - Anti-bot patterns   â”‚
            â”‚   - JS-required pages   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚               â”‚
           Success          Need Browser
                 â”‚               â”‚
                 â”‚               â–¼
                 â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    â”‚    Browser Render Engine    â”‚
                 â”‚    â”‚  - Playwright (Chromium)    â”‚
                 â”‚    â”‚  - Anti-detection scripts   â”‚
                 â”‚    â”‚  - Ad/tracker blocking      â”‚
                 â”‚    â”‚  - Auto-scroll for lazy     â”‚
                 â”‚    â”‚    loading                  â”‚
                 â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚               â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   HTML Processing                       â”‚
â”‚  1. Clean HTML (remove nav, footer, ads, etc.)         â”‚
â”‚  2. Convert to Markdown (Turndown + GFM)                â”‚
â”‚  3. Extract Links (resolve relative URLs)               â”‚
â”‚  4. Extract Metadata (title, description, OG, etc.)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Output                             â”‚
â”‚  {                                                      â”‚
â”‚    success: true,                                       â”‚
â”‚    markdown: "...",                                     â”‚
â”‚    links: ["https://...", ...],                         â”‚
â”‚    metadata: { title: "...", ... },                     â”‚
â”‚    engine: "fetch" | "browser"                          â”‚
â”‚  }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Anti-Detection Features

### HTTP Engine
- Randomized User-Agent from a pool of real browser strings
- Realistic request headers (Accept, Accept-Language, etc.)
- Proper Sec-Fetch-* headers

### Browser Engine
- Override `navigator.webdriver` to prevent detection
- Randomized User-Agent matching browser
- Block ad/tracking domains to speed up loading
- Block unnecessary resource types (media, fonts)
- Simulate human scrolling behavior
- Proper Chrome/browser properties

## Project Structure

```
simple-crawler/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts           # Main entry point
â”‚   â”œâ”€â”€ types.ts           # Type definitions
â”‚   â”œâ”€â”€ demo.ts            # Demo script
â”‚   â”œâ”€â”€ engines/
â”‚   â”‚   â”œâ”€â”€ http.ts        # HTTP fetch engine
â”‚   â”‚   â”œâ”€â”€ browser.ts     # Playwright browser engine
â”‚   â”‚   â””â”€â”€ user-agent.ts  # User-Agent generator
â”‚   â””â”€â”€ processors/
â”‚       â”œâ”€â”€ html-cleaner.ts # HTML content cleaning
â”‚       â”œâ”€â”€ markdown.ts     # HTML to Markdown conversion
â”‚       â”œâ”€â”€ links.ts        # Link extraction
â”‚       â””â”€â”€ metadata.ts     # Metadata extraction
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

## Running the Demo

```bash
pnpm demo
```

## Building

```bash
pnpm build
```

## License

MIT
